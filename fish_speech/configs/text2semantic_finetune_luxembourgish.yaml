defaults:
  - base
  - _self_

project: text2semantic_finetune_luxembourgish
max_length: 4096
pretrained_ckpt_path: checkpoints/openaudio-s1-mini

# Lightning Trainer
trainer:
  accumulate_grad_batches: 5
  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"
  max_steps: 10000
  precision: bf16-true
  limit_val_batches: 10
  val_check_interval: 1000

# Dataset Configuration
tokenizer:
  _target_: fish_speech.tokenizer.FishTokenizer
  model_path: ${pretrained_ckpt_path}/tokenizer.tiktoken

# Dataset Configuration
train_dataset:
  _target_: fish_speech.datasets.semantic.AutoTextSemanticInstructionIterableDataset
  proto_files:
    - data/protos
  tokenizer: ${tokenizer}
  causal: true
  max_length: ${max_length}
  use_speaker: false
  interactive_prob: 0.7

val_dataset:
  _target_: fish_speech.datasets.semantic.AutoTextSemanticInstructionIterableDataset
  proto_files:
    - data/protos
  tokenizer: ${tokenizer}
  causal: true
  max_length: ${max_length}
  use_speaker: false
  interactive_prob: 0.7

data:
  _target_: fish_speech.datasets.semantic.SemanticDataModule
  train_dataset: ${train_dataset}
  val_dataset: ${val_dataset}
  num_workers: 4
  batch_size: 2
  tokenizer: ${tokenizer}
  max_length: ${max_length}

# Model Configuration
model:
  _target_: fish_speech.models.text2semantic.lit_module.TextToSemantic
  model: 
    _target_: fish_speech.models.text2semantic.llama.BaseTransformer.from_pretrained
    path: ${pretrained_ckpt_path}
    load_weights: true
    use_kv_cache: false
    max_length: ${max_length}
    lora_config: null

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1e-4
    weight_decay: 0
    betas: [0.9, 0.95]
    eps: 1e-5

  lr_scheduler:
    _target_: torch.optim.lr_scheduler.LambdaLR
    _partial_: true
    lr_lambda:
      _target_: fish_speech.scheduler.get_constant_schedule_with_warmup_lr_lambda
      _partial_: true
      num_warmup_steps: 10

# Callbacks
callbacks:
  model_checkpoint:
    dirpath: ${paths.ckpt_dir}
    every_n_train_steps: 1000  # Save every 1000 steps
    save_top_k: 6  # Keep more initially, will clean up to 4 (2000, 4000, + 2 best) after training
    save_last: true
    monitor: null
    mode: max
    save_on_train_epoch_end: false
  sample_generation:
    _target_: fish_speech.callbacks.sample_preview.SampleGenerationCallback
    sample_text: "Moien, ech sinn OpenAudio. Ech léieren elo fléissend Lëtzebuergesch ze schwätzen."
    output_dir: ${paths.run_dir}/samples
    decoder_config_name: modded_dac_vq
    decoder_checkpoint_path: checkpoints/openaudio-s1-mini/codec.pth
    every_n_steps: 1000
    temperature: 0.7
    top_p: 0.7
    repetition_penalty: 1.35
    max_new_tokens: 512
    chunk_length: 300
